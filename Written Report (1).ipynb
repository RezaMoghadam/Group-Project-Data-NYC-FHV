{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6470235d-945c-426f-b2d5-dc55b5d4837d",
   "metadata": {},
   "source": [
    "# DCS 232 - Group Project\n",
    "**Dataset**: NYC For-Hire Vehicle (FHV) – Uber, Lyft, Juno, Via\n",
    "# Written Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6145f1b-f92d-42e1-af35-6f24c2afea87",
   "metadata": {},
   "source": [
    "## **1. Introduction to the project**\n",
    "\n",
    "Our project focuses on analyzing the NYC High Volume For-Hire Vehicle (HVFHV) trip dataset, which contains detailed records of ride-hailing trips completed by Uber, Lyft, Via, and Juno. We selected this dataset because of its richness in real-world transportation data and its potential to uncover valuable insights into urban mobility patterns. The dataset spans millions of trips across New York City and captures essential details such as pickup/drop-off times, trip distances, driver pay, tips, and more.\n",
    "\n",
    "This project is particularly compelling because it blends data science with one of the most widely used services in urban life: ride-sharing. Ride-hailing services have dramatically transformed how people commute in large cities, and understanding their behavior through data allows us to identify trends, inefficiencies, and opportunities for optimization. Having a good predictive model in this domain can yield broad societal and economic impacts. For example, it can help ride-sharing platforms optimize driver allocation, reduce passenger wait times, and improve fare structures. It also supports city planners in managing traffic flow and infrastructure development. Moreover, accurate prediction of fares and driver earnings contributes to fairness and transparency within gig economy platforms.\n",
    "\n",
    "Our project aims to explore key patterns in ride-sharing behavior using visual analysis and set the stage for potential predictive modeling. Insights derived from this work can serve as a foundation for further machine learning applications in transportation and mobility services."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f095862-d70c-4fd7-9ef6-2b3036a1eab1",
   "metadata": {},
   "source": [
    "## **2.Figures** \n",
    "\n",
    "To support our exploratory data analysis (EDA) and provide visual insights into the NYC High Volume For-Hire Vehicle (HVFHV) trip data, we present the following figures:\n",
    "\n",
    "**Figure 1: Number of Rides per Hour of Day**\n",
    "\n",
    "This line graph illustrates the distribution of ride volumes across different hours of the day. The data reveals distinct peaks during morning (8–9 AM) and evening (5–6 PM) hours, corresponding to typical commuting times. A noticeable dip occurs during late-night hours, reflecting reduced demand.\n",
    "\n",
    "**Figure 2: Ride Volume by Hour and Weekday**\n",
    "\n",
    "A heatmap showcasing ride volumes segmented by hour and day of the week. Weekdays exhibit higher ride volumes during rush hours, while weekends show increased activity in the late evening, indicating shifts in rider behavior based on the day.\n",
    "\n",
    "**Figure 3: Average Trip Distance and Fare by Hour**\n",
    "\n",
    "This dual-axis chart plots average trip distances and corresponding fares against each hour of the day. Longer trips and higher fares are observed during off-peak hours, suggesting that riders may travel longer distances when traffic is lighter.\n",
    "\n",
    "**Figure 4: Trip Distance vs. Tip Amount**\n",
    "\n",
    "A scatter plot analyzing the relationship between trip distance and tip amounts. The visualization indicates a positive correlation, where longer trips tend to result in higher tips, although variability exists.\n",
    "\n",
    "**Figure 5: Trip Duration vs. Driver Pay**\n",
    "\n",
    "This scatter plot examines how trip duration impacts driver earnings. The data suggests that longer trip durations generally lead to increased driver pay, but with diminishing returns beyond a certain point.\n",
    "\n",
    "**Figure 6: Trip Distance vs. Driver Pay**\n",
    "\n",
    "A scatter plot depicting the association between trip distance and driver compensation. A strong positive correlation is evident, highlighting that longer distances contribute significantly to driver earnings.\n",
    "\n",
    "Each figure includes appropriate legends and annotations to enhance interpretability. These visualizations collectively provide a comprehensive overview of ride patterns, financial aspects, and temporal dynamics within NYC's for-hire vehicle sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd6dee-dc2e-4ede-a3ac-c313f6f6bc9f",
   "metadata": {},
   "source": [
    "## 3. **Methods Section**\n",
    "\n",
    "**1. Data Exploration**\n",
    "\n",
    "We began by loading the NYC High Volume For-Hire Vehicle (HVFHV) dataset from 2019 onward, using PySpark for scalable data processing. A Spark session was created with appropriate memory configurations to handle the large dataset efficiently. Initial exploration included examining schema structure, sample rows, column names, and total row counts. Certain columns such as flags and the airport_fee were dropped due to mixed data types or irrelevance to prediction.\n",
    "\n",
    "**2. Preprocessing**\n",
    "\n",
    "We selected key numerical features relevant to ride behavior: pickup_datetime, dropoff_datetime, trip_miles, trip_time, base_passenger_fare, driver_pay, and location IDs. Datetime columns were transformed to extract hour and weekday components. We also performed feature engineering to create useful features like fare_per_mile, and removed outliers based on trip duration and distance.\n",
    "Further preprocessing included converting categorical columns to numerical representations and assembling the feature vector using Spark’s VectorAssembler.\n",
    "\n",
    "**3. Model Training:** \n",
    "\n",
    "**Linear Regression, GBT Regressor, Random Forest**\n",
    "\n",
    "We selected the Linear Regression (LR) model as our baseline because it is simple, interpretable, and computationally efficient. It allows us to establish a reference point for evaluating the performance of more complex models. Also, our LR model provides insight into how each feature influences the target variable (driver pay), helping us understand the data relationships before applying advanced methods.\n",
    "\n",
    "We trained a baseline Linear Regression model using Spark’s MLlib to predict driver pay. The model was trained on engineered features such as trip duration, distance, pickup/dropoff zones, and time-of-day indicators.\n",
    "''' Link to our code'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27721f35-0eac-4ce0-9f2c-204ef2facf77",
   "metadata": {},
   "source": [
    "## **4. Result Section**\n",
    "\n",
    "**1. Data Exploration**\n",
    "From our exploratory analysis, we observed the following patterns:\n",
    "- Ride demand peaks during evening hours across all weekdays.\n",
    "- Average trip distance and fare steadily increase during late-night hours.\n",
    "- Longer trips tend to yield higher tips and greater driver pay.\n",
    "- Variability in driver pay is more pronounced during longer trip durations.\n",
    "\n",
    "**2. Preprocessing**\n",
    "- Missing values in fare components were removed.\n",
    "- Categorical variables such as pickup and drop-off locations were encoded using one-hot encoding.\n",
    "- Features like trip_miles, trip_time, and tips were scaled using standard normalization.\n",
    "\n",
    "**3. Model 1: Linear Regression**\n",
    "- Linear Regression was used to model the relationship between trip characteristics and driver pay.\n",
    "- The model was trained on the full pipeline with all selected features.\n",
    "- Performance Metrics:\n",
    " - RMSE: 3.3168\n",
    " - R² Score: 0.8021\n",
    "\n",
    "**4. Model 2: GBT Regressor (Gradient Boosted Trees)**\n",
    "- Gradient Boosted Trees were implemented to capture non-linear relationships.\n",
    "- The model was configured with 100 iterations, a learning rate of 0.1, and max depth of 5.\n",
    "- Performance Metrics:\n",
    "- RMSE: 3.0578 \n",
    "- R² Score: 0.8318\n",
    "\n",
    "**5. Model 3. Random Forest**\n",
    "- Random Forest was utilized to model complex patterns by aggregating predictions from multiple decision trees.\n",
    "- The model was configured with 100 trees and a maximum depth of 10 to balance accuracy and overfitting.\n",
    "- Performance Metrics:\n",
    "- RMSE: 3.0289\n",
    "- R² Score: 0.8350\n",
    "\n",
    "\n",
    "(Additional model results will be added as the project progresses.)\n",
    "\n",
    "**Figures Included:**\n",
    "- Predicted vs. Actual Driver Pay (Linear Regression)\n",
    "- Feature Importance Plot (GBT Regressor)\n",
    "- Predicted Driver Pay (Random Forest)\n",
    "- Residual Plot (for all 3 models)\n",
    "- RMSE comparison bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e84414-bbcf-48b9-b040-091a81743103",
   "metadata": {},
   "source": [
    "## **5. Discussion Section**\n",
    "\n",
    "This project began with the goal of understanding the economic patterns of NYC for-hire vehicle services using predictive modeling. We recognized the broader implications of being able to accurately model ride behavior, particularly in terms of driver compensation and platform efficiency. A well-performing model can be crucial not just for businesses seeking optimization, but also for supporting fair wage systems and identifying potential inequities in pricing or pay distribution.\n",
    "\n",
    "**Challenges we faced.**   \n",
    "**SDSC.**  One of the early challenges we encountered was connecting to the San Diego Supercomputer Center (SDSC). The initial setup—configuring secure connections, managing port forwarding for Jupyter Notebooks, and aligning dependencies—proved more complicated than expected. While time-consuming, this process highlighted the importance of robust infrastructure and served as a valuable learning experience in using scalable computational resources. Once set up, SDSC gave us the ability to experiment with more complex models without worrying about local processing limitations.   \n",
    "\n",
    "**Model 1.** The first model training session also presented practical issues. Our preprocessing step introduced a high number of features, especially due to one-hot encoding of location-based variables. This caused unexpected memory issues and long training times, even with relatively simple models like Linear Regression. We had to restructure our pipeline to process data in batches and manage feature dimensionality more carefully to reduce runtime without sacrificing accuracy.  \n",
    "Our first model Linear Regression served as a useful baseline, it quickly became apparent that it could not capture the nuanced relationships in our data. It underperformed especially on longer or irregular trips, and the residual errors suggested significant underfitting. In contrast, Gradient Boosted Trees (GBT) showed a substantial improvement in performance. Its ability to model non-linear interactions and variable importance helped us gain insights into the most influential features affecting driver pay and tips.   \n",
    "**Futures Label Leakage**. Our results were promising, showing that models could predict driver pay and fare components with reasonably high accuracy. But we also recognized limitations. For instance, some features might be exhibiting label leakage, and we have not yet addressed potential biases related to geography, time, or socioeconomic status. These areas will require further analysis and possibly the integration of external data sources like weather or traffic information.\n",
    "\n",
    "This project has been an exercise not just in machine learning, but in iterative problem-solving. From data management hurdles to model refinement, we approached each stage critically, aiming to balance technical rigor with interpretability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4332c8-83d0-4bfe-86a6-ceb417402813",
   "metadata": {},
   "source": [
    "## 6. **Conclusion**\n",
    "\n",
    "Working on this project has been a rewarding experience that offered us insight not only into the inner workings of NYC’s for-hire vehicle economy but also into the practical challenges of real-world machine learning pipelines. If we were to do it again, there are definitely things we would have approached differently.\n",
    "\n",
    "First, we would have prioritized building a more flexible data pipeline early on. The large feature set and high cardinality variables slowed us down during model iterations and made it harder to experiment with more advanced models. Additionally, integrating external factors such as weather conditions, traffic data, or event schedules could significantly improve the predictive power of our models by adding real-world context to numerical features like trip duration and driver pay.\n",
    "\n",
    "We also wish we had more time to explore deep learning models or hybrid approaches. While our models like Linear Regression and Gradient Boosted Trees gave us valuable baselines and decent accuracy, a deeper neural network model might be able to capture more abstract relationships between features like location and tipping behavior.\n",
    "\n",
    "Another future direction would be incorporating explainability methods such as SHAP values or LIME to better understand feature contributions, especially for complex models like GBTs. This would not only enhance model transparency but also aid in identifying unintended biases or overfitting risks.   \n",
    "We see an opportunity to turn this into a more interactive dashboard or decision-support tool for drivers or policymakers. If we could deploy our models in real-time with daily updated data, this could help optimize route planning, incentive structuring, or dynamic pricing strategies across ride-hailing platforms.\n",
    "\n",
    "In closing, this project reinforced how crucial it is to bridge technical rigor with domain understanding. Data alone isn’t enough—it’s about interpreting that data meaningfully and responsibly. While we made solid progress, there’s plenty of room for future exploration, and we’re excited about where this work could go next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb52831-8e93-4403-8275-4e132560ab44",
   "metadata": {},
   "source": [
    "## **7. Statement of Collaboration**\n",
    "\n",
    "**Reza Moghada**  \n",
    "Title: Team Leader & Coding   \n",
    "Contribution: Reza led the overall project coordination and timeline management. He worked on connecting to the SDSC cluster, preparing the environment, and resolving issues with data uploads and access. He also facilitated communication among team members, managed task delegation, and was actively involved in all parts of the project—from data exploration to model evaluation. He also oversaw the integration of final results and helped refine the written report.\n",
    "\n",
    "**Aryslan Vakilov**  \n",
    "Title: Coding & Technical Writer  \n",
    "Contribution: Aryslan focused on model development, particularly the implementation and evaluation of the Gradient Boosted Trees and Random Forest models. He also conducted extensive exploratory data analysis (EDA) of the RMSE and R2 for model performances, contributed to the visualization components, and authored significant portions of the methods, results, and preparing the written report.\n",
    "\n",
    "**Kyle Parker**  \n",
    "Title: Machine Learning Engineer   \n",
    "Contribution: Kyle was responsible for experimenting with advanced models such as Linear Regression model. He fine-tuned model parameters and performed comparative performance analysis across models. Kyle also contributed to feature engineering and helped troubleshoot technical issues with data preprocessing.\n",
    "\n",
    "**Dennis Krutkin**  \n",
    "Title: Data and Machine Learning Engineer   \n",
    "Contribution: Dennis handled data cleaning and preprocessing pipelines. He also contributed to the setup of the project’s infrastructure and documented the data processing steps. He helped to open discord channel and organizing zoom meetings throughout the project.\n",
    "\n",
    "As a team, we collaborated effectively through regular meetings, shared Git repositories, and coordinated progress via our Discord channel for group chat. We ensured all members were aware of the project’s direction and goals. While Reza served as the team leader, all team members contributed meaningfully and equitably to the project’s success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a40aac-97f8-418c-84be-79ec5ef4de8d",
   "metadata": {},
   "source": [
    "## **8. Final Model, Results Summary and Github Repository**  \n",
    "\n",
    "- **Final Model and Results Summary**\n",
    "\n",
    "The Random Forest Regressor was selected as the final model for this project due to its superior performance in predicting driver pay. By leveraging 100 decision trees with a maximum depth of 10, the model effectively captured complex, non-linear relationships in the dataset. Compared to other models such as Linear Regression and Gradient Boosted Trees, the Random Forest achieved the lowest **RMSE** of **3.0289** and the highest **R² score** of **0.8350**, indicating strong predictive accuracy and a well-fitted model. Its robustness to overfitting and ability to handle diverse feature types made it the most effective choice for our ride-sharing trip analysis.\n",
    "\n",
    "- GitHub Repository\n",
    "\n",
    "Your GitHub must be made public by the morning of the next day of the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355693f-5174-4e90-8150-163dc8917763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
